{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision tree implementation from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9YbvHG1EhXc"
      },
      "source": [
        "# Building the decision tree classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW7_db94tvXP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Enter You Name Here\n",
        "myname = \"Tamal-Mondal-\"\n",
        "\n",
        "# Writing results to a file (DO NOT CHANGE)\n",
        "f = open(myname + \"result.txt\", \"w\")\n",
        "\n",
        "# Structure of a single node in the decision tree\n",
        "class TreeNode():\n",
        "    def __init__(self, left_child=None, right_child=None, feature=None, threshold_value = None, class_label=None):\n",
        "        \n",
        "        # for internal or decision node\n",
        "        self.left_child = left_child\n",
        "        self.right_child = right_child\n",
        "        self.feature = feature\n",
        "        self.threshold_value = threshold_value\n",
        "        \n",
        "        # This is only applicable to leaf node to capture the class label\n",
        "        self.class_label = class_label\n",
        "\n",
        "class DecisionTree():\n",
        "    def __init__(self, information_gain_strategy, minimum_datapoints_to_split, maximum_height):\n",
        "        self.root = None\n",
        "        self.information_gain_strategy = information_gain_strategy\n",
        "\n",
        "        # These are the parameters to do pre-pruning to avoid overfitting\n",
        "        self.minimum_datapoints_to_split = minimum_datapoints_to_split\n",
        "        self.maximum_height = maximum_height\n",
        "\n",
        "    # Method that recursively gets called and builds the decision tree\n",
        "    def generate_decision_tree(self, datapoints, current_height):\n",
        "        number_of_data_points = np.shape(datapoints)[0]\n",
        "\n",
        "        # Only try to split if the tree has not reached maximum height and there are sufficient no of data points\n",
        "        if number_of_data_points >= self.minimum_datapoints_to_split and current_height < self.maximum_height:\n",
        "\n",
        "            # Find the best split that maximizes the information gain\n",
        "            spliting_details = self.find_best_split_with_threshold(datapoints)\n",
        "\n",
        "            # Split only if there is valid information gain\n",
        "            if spliting_details[\"information_gain\"] > 0:\n",
        "                right_tree = self.generate_decision_tree(spliting_details[\"datapoints_to_right\"], current_height + 1)\n",
        "                left_tree = self.generate_decision_tree(spliting_details[\"datapoints_to_left\"], current_height + 1)\n",
        "                return TreeNode(left_tree, right_tree, spliting_details[\"feature\"], spliting_details[\"threshold_value\"])\n",
        "        \n",
        "        # Otherwise create a leaf node by majority count\n",
        "        return TreeNode(class_label = find_class_label_by_majority_vote(datapoints[:,-1]))\n",
        "\n",
        "    # Method to try all features and their thresolds to find \n",
        "    # the best way to split depending on information gain\n",
        "    def find_best_split_with_threshold(self, datapoints):\n",
        "\n",
        "        # It stores the details related to best split\n",
        "        spliting_details = {\"information_gain\" : float(-np.inf)}\n",
        "        \n",
        "        # Check all unique feature values as thresolds for all the features\n",
        "        for index in range(np.shape(datapoints)[1] - 1):\n",
        "            for threshold_value in np.unique(datapoints[:, index]):\n",
        "                \n",
        "                # Split the dataset by thresold for a given feature\n",
        "                datapoints_to_left = np.array([datapoint for datapoint in datapoints if datapoint[index] <= threshold_value])\n",
        "                datapoints_to_right = np.array([datapoint for datapoint in datapoints if datapoint[index] > threshold_value])\n",
        "\n",
        "                # Modify split details if a better information gain is found\n",
        "                if len(datapoints_to_left) >= 1 and len(datapoints_to_right) >= 1:\n",
        "                    information_gain = self.calculate_information_gain(datapoints[:, -1], datapoints_to_left[:, -1], datapoints_to_right[:, -1])\n",
        "                    if information_gain > spliting_details[\"information_gain\"]:\n",
        "                        spliting_details[\"information_gain\"]  = information_gain\n",
        "                        spliting_details[\"datapoints_to_left\"] = datapoints_to_left\n",
        "                        spliting_details[\"datapoints_to_right\"] = datapoints_to_right\n",
        "                        spliting_details[\"feature\"] = index\n",
        "                        spliting_details[\"threshold_value\"] = threshold_value\n",
        "        return spliting_details\n",
        "\n",
        "    # Method to compute information gain by \"entropy\" or \"gini index\"\n",
        "    def calculate_information_gain(self, y_parent, y_left_tree, y_right_tree):\n",
        "        left_child_weight = len(y_left_tree) / len(y_parent)\n",
        "        right_child_weight = len(y_right_tree) / len(y_parent)\n",
        "        information_gain = float(-np.inf)\n",
        "        if self.information_gain_strategy == \"entropy\":\n",
        "            information_gain = compute_entropy(y_parent) - (left_child_weight * compute_entropy(y_left_tree) + right_child_weight * compute_entropy(y_right_tree))\n",
        "        elif self.information_gain_strategy == \"gini\":\n",
        "            information_gain = compute_gini_index(y_parent) - (left_child_weight * compute_gini_index(y_left_tree) + right_child_weight * compute_gini_index(y_left_tree))\n",
        "        return information_gain\n",
        "\n",
        "    # Method to recursively traverse the decision tree and make prediction for a single data point\n",
        "    def predict_class(self, datapoint, node):\n",
        "\n",
        "        # Reached leaf\n",
        "        if node.class_label != None: \n",
        "          return node.class_label\n",
        "\n",
        "        # Otherwise look into the left or right subtree\n",
        "        if datapoint[node.feature] <= node.threshold_value:\n",
        "            return self.predict_class(datapoint, node.left_child)\n",
        "        else:\n",
        "            return self.predict_class(datapoint, node.right_child)\n",
        "    \n",
        "    # Method to start training or building the decision tree\n",
        "    def learn(self, training_set):\n",
        "        self.root = self.generate_decision_tree(training_set, 0)\n",
        "    \n",
        "    # Method to predict class for a test instance\n",
        "    def classify(self, test_instance):\n",
        "        return self.predict_class(test_instance, self.root)\n",
        "\n",
        "    # Method to post-prune the decision tree\n",
        "    def post_prune(self, datapoints, node):\n",
        "\n",
        "        # If it's a leaf, nothing to do\n",
        "        if node.class_label != None: \n",
        "            return \n",
        "\n",
        "        # For a decision-node, split the data by feature\n",
        "        datapoints_to_left = np.array([datapoint for datapoint in datapoints if datapoint[node.feature] <= node.threshold_value])\n",
        "        datapoints_to_right = np.array([datapoint for datapoint in datapoints if datapoint[node.feature] > node.threshold_value])\n",
        "\n",
        "        # Prune the left and right sub-tree first\n",
        "        self.post_prune(datapoints_to_left, node.left_child)\n",
        "        self.post_prune(datapoints_to_right, node.right_child)\n",
        "\n",
        "        # Prune the current node if necessary\n",
        "        if(node.left_child.class_label != None and node.right_child.class_label != None):\n",
        "\n",
        "            # Miss-classification error with pruning\n",
        "            class_label_after_pruning = node.left_child.class_label if len(datapoints_to_left) > len(datapoints_to_right) else node.right_child.class_label\n",
        "            miss_classification_after_pruning = (datapoints[:, -1] != class_label_after_pruning).sum() if len(datapoints) > 0 else 0\n",
        "            \n",
        "            # Miss-classification error without pruning\n",
        "            miss_classification_before_pruning = (datapoints_to_left[:, -1] != node.left_child.class_label).sum() if len(datapoints_to_left) > 0 else 0\n",
        "            miss_classification_before_pruning = miss_classification_before_pruning + ((datapoints_to_right[:, -1] != node.right_child.class_label).sum() if len(datapoints_to_right) > 0 else 0)\n",
        "            \n",
        "            # Prune if miss-classification error is less or equal after pruning\n",
        "            if(miss_classification_after_pruning <= miss_classification_before_pruning):\n",
        "              node.class_label = class_label_after_pruning\n",
        "              node.left_child = None\n",
        "              node.right_child = None\n",
        "              return\n",
        "    \n",
        "    # Method that returns height of the decision tree\n",
        "    def get_height(self, node):\n",
        "        if node == None or node.class_label != None:\n",
        "            return 0\n",
        "        else:\n",
        "            return max(self.get_height(node.left_child), self.get_height(node.right_child)) + 1\n",
        "\n",
        "####################################################################################################################\n",
        "\n",
        "# These are some of the utility/library methods which are not specific to any Decision Tree\n",
        "\n",
        "# Utility method to compute Entropy\n",
        "def compute_entropy(y):\n",
        "  entropy = 0\n",
        "  class_counts = np.asarray(np.unique(y, return_counts=True)).T\n",
        "  for class_count in class_counts:\n",
        "    class_probability = class_count[1] / len(y)\n",
        "    entropy = entropy + ((-1) * class_probability * np.log2(class_probability))\n",
        "  return entropy\n",
        "\n",
        "# Utility method to compute Gini index\n",
        "def compute_gini_index(y):\n",
        "  gini_impurity = 0\n",
        "  class_counts = np.asarray(np.unique(y, return_counts=True)).T\n",
        "  for class_count in class_counts:\n",
        "    class_probability = class_count[1] / len(y)\n",
        "    gini_impurity = gini_impurity + (class_probability * class_probability)\n",
        "  return (1 - gini_impurity)\n",
        "\n",
        "# Utility method that returns the majority class\n",
        "def find_class_label_by_majority_vote(y):\n",
        "  majority_class, majority_class_count = -1, -1\n",
        "  class_counts = np.asarray(np.unique(y, return_counts=True)).T\n",
        "  for class_count in class_counts:\n",
        "    majority_class = class_count[0] if class_count[1] > majority_class_count else majority_class\n",
        "    majority_class_count = class_count[1] if class_count[1] > majority_class_count else majority_class_count\n",
        "  return majority_class\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY9c1GGdEwV7"
      },
      "source": [
        "# K-fold cross validation and accuracy calculation\n",
        "\n",
        "#### Using \n",
        "#### K = 10\n",
        "#### Impurity measurement function = entropy\n",
        "#### Maximum height of the tree = Infinity(to avoid any early stopping)\n",
        "#### Minimum samples to split = 2(to avoid any early stopping)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIcz4WtBoX4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e269d71-3633-42c0-8d24-6b679091926d"
      },
      "source": [
        "import math\n",
        "\n",
        "# Method that splits the data in \"K\" equals portions for Cross-Validation\n",
        "def KFolds_with_stratified_sampling(dataset, no_of_folds):\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(dataset) \n",
        "    folds = []\n",
        "    fold_size = int(len(dataset) / no_of_folds)\n",
        "    class1_dataset, class2_dataset = dataset[np.where(dataset[:, -1] == 1)], dataset[np.where(dataset[:, -1] == 0)]\n",
        "    class1_size_per_fold, class2_size_per_fold = int(fold_size * (len(class1_dataset) / len(dataset))), int(fold_size * (len(class2_dataset) / len(dataset)))\n",
        "\n",
        "    # Create \"K\" folds maintaining same class proportions(stratified sampling)\n",
        "    for i in range(no_of_folds):\n",
        "        class1_data = class1_dataset[i*class1_size_per_fold : (i+1)*class1_size_per_fold]\n",
        "        class2_data = class2_dataset[i*class2_size_per_fold : (i+1)*class2_size_per_fold]\n",
        "        fold = np.concatenate((class1_data, class2_data), axis=0)\n",
        "        np.random.seed(42)\n",
        "        np.random.shuffle(fold) \n",
        "        folds.append(fold)\n",
        "    return folds\n",
        "\n",
        "# Method to train and test the decision tree using K-fold cross-validation\n",
        "def run_decision_tree(no_of_folds, information_gain_strategy, minimum_datapoints_to_split, maximum_height):\n",
        "\n",
        "    # Load data set\n",
        "    wine_dataset = pd.read_csv(\"wine-dataset.csv\")\n",
        "    print(wine_dataset.describe())\n",
        "    print(wine_dataset.shape)\n",
        "\n",
        "    # Split the dataset accordingly for cross-validation\n",
        "    global f\n",
        "    folds = KFolds_with_stratified_sampling(np.asarray(wine_dataset), no_of_folds)\n",
        "    accuracies_for_all_folds = []\n",
        "\n",
        "    # For K-Fold cross-validation, we need to train and validate \"K\" times\n",
        "    for i in range(no_of_folds):\n",
        "        decision_tree = DecisionTree(information_gain_strategy, minimum_datapoints_to_split, maximum_height)\n",
        "\n",
        "        # Build the training dataset from other (K-1) splits\n",
        "        training_data = []\n",
        "        for j in range(i):\n",
        "          if(len(training_data) == 0):\n",
        "            training_data = folds[j]\n",
        "          else:\n",
        "            training_data = np.concatenate((training_data, folds[j]), axis=0)\n",
        "        for j in range(i+1, no_of_folds):\n",
        "          if(len(training_data) == 0):\n",
        "            training_data = folds[j]\n",
        "          else:\n",
        "            training_data = np.concatenate((training_data, folds[j]), axis=0)\n",
        "\n",
        "        # Train the decision tree\n",
        "        decision_tree.learn(training_data)\n",
        "        f.write(\"Height of the tree for fold-{} : {}\\n\".format(i+1, decision_tree.get_height(decision_tree.root)))\n",
        "\n",
        "        # Test the model using ith split\n",
        "        results = []\n",
        "        for instance in folds[i]:\n",
        "            result = decision_tree.classify(instance)\n",
        "            results.append( result == instance[-1])\n",
        "        \n",
        "        # Accuracy for each fold\n",
        "        accuracy = float(results.count(True))/float(len(results))\n",
        "        print(\"Accuracy for fold-{} is {}\".format(i+1, accuracy))\n",
        "        f.write(\"Accuracy for fold-{} is {}\\n\".format(i+1, accuracy))\n",
        "        accuracies_for_all_folds.append(accuracy)       \n",
        "    \n",
        "    # Calculate average accuracy\n",
        "    print(\"Average accuracy: %.4f\" % float(sum(accuracies_for_all_folds)/len(accuracies_for_all_folds)))\n",
        "    f.write(\"Average accuracy: %.4f\\n\" % float(sum(accuracies_for_all_folds)/len(accuracies_for_all_folds)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    f.write(\"\\n=============================10 fold cross-Validation results for the decision tree\\n\")\n",
        "    run_decision_tree(10, \"entropy\", 2, math.inf)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
            "count    4898.000000       4898.000000  ...  4898.000000  4898.000000\n",
            "mean        6.854788          0.278241  ...    10.514267     0.216415\n",
            "std         0.843868          0.100795  ...     1.230621     0.411842\n",
            "min         3.800000          0.080000  ...     8.000000     0.000000\n",
            "25%         6.300000          0.210000  ...     9.500000     0.000000\n",
            "50%         6.800000          0.260000  ...    10.400000     0.000000\n",
            "75%         7.300000          0.320000  ...    11.400000     0.000000\n",
            "max        14.200000          1.100000  ...    14.200000     1.000000\n",
            "\n",
            "[8 rows x 12 columns]\n",
            "(4898, 12)\n",
            "Accuracy for fold-1 is 0.8360655737704918\n",
            "Accuracy for fold-2 is 0.8565573770491803\n",
            "Accuracy for fold-3 is 0.8237704918032787\n",
            "Accuracy for fold-4 is 0.8360655737704918\n",
            "Accuracy for fold-5 is 0.8299180327868853\n",
            "Accuracy for fold-6 is 0.8504098360655737\n",
            "Accuracy for fold-7 is 0.8401639344262295\n",
            "Accuracy for fold-8 is 0.8586065573770492\n",
            "Accuracy for fold-9 is 0.8278688524590164\n",
            "Accuracy for fold-10 is 0.8360655737704918\n",
            "Average accuracy: 0.8395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM5rjJMwB319"
      },
      "source": [
        "# Check accuracy using Gini index in place of Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL8Ts0XqB3Jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64356307-fa1b-4f76-bbc3-3e992bdb23d4"
      },
      "source": [
        "minimum_samples_to_split = 2\n",
        "maximum_height = 23\n",
        "information_gain_strategy = \"gini\"\n",
        "\n",
        "# We are also applying early stoping to avoid indefinite growth of the tree\n",
        "f.write(\"\\n==========================================Results using Gini index\\n\")\n",
        "run_decision_tree(10, information_gain_strategy, minimum_samples_to_split, maximum_height)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
            "count    4898.000000       4898.000000  ...  4898.000000  4898.000000\n",
            "mean        6.854788          0.278241  ...    10.514267     0.216415\n",
            "std         0.843868          0.100795  ...     1.230621     0.411842\n",
            "min         3.800000          0.080000  ...     8.000000     0.000000\n",
            "25%         6.300000          0.210000  ...     9.500000     0.000000\n",
            "50%         6.800000          0.260000  ...    10.400000     0.000000\n",
            "75%         7.300000          0.320000  ...    11.400000     0.000000\n",
            "max        14.200000          1.100000  ...    14.200000     1.000000\n",
            "\n",
            "[8 rows x 12 columns]\n",
            "(4898, 12)\n",
            "Accuracy for fold-1 is 0.7868852459016393\n",
            "Accuracy for fold-2 is 0.7848360655737705\n",
            "Accuracy for fold-3 is 0.7848360655737705\n",
            "Accuracy for fold-4 is 0.7889344262295082\n",
            "Accuracy for fold-5 is 0.7868852459016393\n",
            "Accuracy for fold-6 is 0.7848360655737705\n",
            "Accuracy for fold-7 is 0.7848360655737705\n",
            "Accuracy for fold-8 is 0.7827868852459017\n",
            "Accuracy for fold-9 is 0.7868852459016393\n",
            "Accuracy for fold-10 is 0.7848360655737705\n",
            "Average accuracy: 0.7857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJPxVbB4j3_H"
      },
      "source": [
        "# Pre-pruning to improve the accuracy\n",
        "\n",
        "### We have tried to restrict the depth of the tree and minimum samples needed to split a node to avoid overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2PtnlD0i6mG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656b0910-dab7-46cb-a670-cfe191af91f0"
      },
      "source": [
        "minimum_samples_to_split = 2\n",
        "maximum_height = 23\n",
        "information_gain_strategy = \"entropy\"\n",
        "\n",
        "f.write(\"\\n====================================Results using Pre-pruning with entropy\\n\")\n",
        "run_decision_tree(10, information_gain_strategy, minimum_samples_to_split, maximum_height)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
            "count    4898.000000       4898.000000  ...  4898.000000  4898.000000\n",
            "mean        6.854788          0.278241  ...    10.514267     0.216415\n",
            "std         0.843868          0.100795  ...     1.230621     0.411842\n",
            "min         3.800000          0.080000  ...     8.000000     0.000000\n",
            "25%         6.300000          0.210000  ...     9.500000     0.000000\n",
            "50%         6.800000          0.260000  ...    10.400000     0.000000\n",
            "75%         7.300000          0.320000  ...    11.400000     0.000000\n",
            "max        14.200000          1.100000  ...    14.200000     1.000000\n",
            "\n",
            "[8 rows x 12 columns]\n",
            "(4898, 12)\n",
            "Accuracy for fold-1 is 0.8360655737704918\n",
            "Accuracy for fold-2 is 0.8565573770491803\n",
            "Accuracy for fold-3 is 0.8278688524590164\n",
            "Accuracy for fold-4 is 0.8360655737704918\n",
            "Accuracy for fold-5 is 0.8299180327868853\n",
            "Accuracy for fold-6 is 0.8545081967213115\n",
            "Accuracy for fold-7 is 0.8401639344262295\n",
            "Accuracy for fold-8 is 0.8586065573770492\n",
            "Accuracy for fold-9 is 0.8278688524590164\n",
            "Accuracy for fold-10 is 0.8360655737704918\n",
            "Average accuracy: 0.8404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JZ0mtXRLQNQ"
      },
      "source": [
        "# Post-pruning based on miss-classification error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onKLEyVRLNJv",
        "outputId": "bc7d5570-2ae6-47f5-c950-5107f9cd5046"
      },
      "source": [
        "\n",
        "# Method to apply post-pruning with cross-validation\n",
        "def run_decision_tree_with_post_pruning(no_of_folds, information_gain_strategy, minimum_datapoints_to_split, maximum_height):\n",
        "\n",
        "    # Load data set\n",
        "    wine_dataset = pd.read_csv(\"wine-dataset.csv\")\n",
        "    print(wine_dataset.describe())\n",
        "    print(wine_dataset.shape)\n",
        "\n",
        "    # Divide the complete dataset and get the K-folds\n",
        "    folds = KFolds_with_stratified_sampling(np.asarray(wine_dataset), no_of_folds)\n",
        "\n",
        "    # Here using the last fold as test set to compare the accuracy\n",
        "    test_data = folds[no_of_folds - 1]\n",
        "\n",
        "    global f\n",
        "    accuracies_after_pruning = []\n",
        "    accuracies_before_pruning = []\n",
        "\n",
        "    # For K-Fold cross-validation, we need to train and validate \"K\" times\n",
        "    for i in range(no_of_folds-1):\n",
        "\n",
        "        pruning_data = folds[i]\n",
        "        training_data = []\n",
        "        for j in range(i):\n",
        "          if(len(training_data) == 0):\n",
        "            training_data = folds[j]\n",
        "          else:\n",
        "            training_data = np.concatenate((training_data, folds[j]), axis=0)\n",
        "        for j in range(i+1, no_of_folds-1):\n",
        "          if(len(training_data) == 0):\n",
        "            training_data = folds[j]\n",
        "          else:\n",
        "            training_data = np.concatenate((training_data, folds[j]), axis=0)\n",
        "\n",
        "        # Build the decision tree\n",
        "        decision_tree = DecisionTree(information_gain_strategy, minimum_datapoints_to_split, maximum_height)\n",
        "\n",
        "        # Train the decision tree\n",
        "        decision_tree.learn(training_data)\n",
        "        f.write(\"Height of the tree before pruning for fold-{} : {}\\n\".format(i+1, decision_tree.get_height(decision_tree.root)))\n",
        "\n",
        "        # Accuracy before post-pruning\n",
        "        results = []\n",
        "        for instance in test_data:\n",
        "            result = decision_tree.classify(instance)\n",
        "            results.append( result == instance[-1])\n",
        "        accuracy = float(results.count(True))/float(len(results))\n",
        "        accuracies_before_pruning.append(accuracy)\n",
        "        print(\"Accuracy before post-pruning is {} for fold-{}\".format(accuracy, i+1))\n",
        "        f.write(\"Accuracy before post-pruning is {} for fold-{}\\n\".format(accuracy, i+1))\n",
        "\n",
        "        # Prune the decision tree\n",
        "        decision_tree.post_prune(data_used_for_pruning, decision_tree.root)\n",
        "        f.write(\"Height of the tree after pruning for fold-{} : {}\\n\".format(i+1, decision_tree.get_height(decision_tree.root)))\n",
        "        \n",
        "        # Accuracy after post-pruning\n",
        "        results = []\n",
        "        for instance in test_data:\n",
        "            result = decision_tree.classify(instance)\n",
        "            results.append( result == instance[-1])\n",
        "        accuracy = float(results.count(True))/float(len(results))\n",
        "        accuracies_after_pruning.append(accuracy)\n",
        "        print(\"Accuracy after post-pruning is {} for fold-{}\".format(accuracy, i+1))\n",
        "        f.write(\"Accuracy after post-pruning is {} for fold-{}\\n\".format(accuracy, i+1))\n",
        "        \n",
        "    # Calculate average accuracy\n",
        "    print(\"Average accuracy before post-pruning: %.4f\" % float(sum(accuracies_before_pruning)/len(accuracies_before_pruning)))\n",
        "    f.write(\"Average accuracy before post-pruning: %.4f\\n\" % float(sum(accuracies_before_pruning)/len(accuracies_before_pruning)))\n",
        "    print(\"Average accuracy after post-pruning: %.4f\" % float(sum(accuracies_after_pruning)/len(accuracies_after_pruning)))\n",
        "    f.write(\"Average accuracy after post-pruning: %.4f\\n\" % float(sum(accuracies_after_pruning)/len(accuracies_after_pruning)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    f.write(\"\\n=====================================Results using Post-pruning with entropy\\n\")\n",
        "    run_decision_tree_with_post_pruning(10, \"entropy\", 2, math.inf)\n",
        "    f.close()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
            "count    4898.000000       4898.000000  ...  4898.000000  4898.000000\n",
            "mean        6.854788          0.278241  ...    10.514267     0.216415\n",
            "std         0.843868          0.100795  ...     1.230621     0.411842\n",
            "min         3.800000          0.080000  ...     8.000000     0.000000\n",
            "25%         6.300000          0.210000  ...     9.500000     0.000000\n",
            "50%         6.800000          0.260000  ...    10.400000     0.000000\n",
            "75%         7.300000          0.320000  ...    11.400000     0.000000\n",
            "max        14.200000          1.100000  ...    14.200000     1.000000\n",
            "\n",
            "[8 rows x 12 columns]\n",
            "(4898, 12)\n",
            "Accuracy before post-pruning is 0.8381147540983607 for fold-1\n",
            "Accuracy after post-pruning is 0.8442622950819673 for fold-1\n",
            "Accuracy before post-pruning is 0.8545081967213115 for fold-2\n",
            "Accuracy after post-pruning is 0.8463114754098361 for fold-2\n",
            "Accuracy before post-pruning is 0.8442622950819673 for fold-3\n",
            "Accuracy after post-pruning is 0.8545081967213115 for fold-3\n",
            "Accuracy before post-pruning is 0.8114754098360656 for fold-4\n",
            "Accuracy after post-pruning is 0.8176229508196722 for fold-4\n",
            "Accuracy before post-pruning is 0.8381147540983607 for fold-5\n",
            "Accuracy after post-pruning is 0.8340163934426229 for fold-5\n",
            "Accuracy before post-pruning is 0.8401639344262295 for fold-6\n",
            "Accuracy after post-pruning is 0.8319672131147541 for fold-6\n",
            "Accuracy before post-pruning is 0.8176229508196722 for fold-7\n",
            "Accuracy after post-pruning is 0.8073770491803278 for fold-7\n",
            "Accuracy before post-pruning is 0.8032786885245902 for fold-8\n",
            "Accuracy after post-pruning is 0.805327868852459 for fold-8\n",
            "Accuracy before post-pruning is 0.8094262295081968 for fold-9\n",
            "Accuracy after post-pruning is 0.8258196721311475 for fold-9\n",
            "Average accuracy before post-pruning: 0.8286\n",
            "Average accuracy after post-pruning: 0.8297\n"
          ]
        }
      ]
    }
  ]
}