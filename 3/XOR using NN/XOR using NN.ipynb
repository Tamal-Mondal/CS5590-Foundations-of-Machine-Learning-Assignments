{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.4","file_extension":".py","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# We've heard the folklore of \"Deep Learning\" solved the XOR problem.\n\nThe XOR problem is known to be solved by the multi-layer perceptron given all 4 boolean inputs and outputs, it trains and memorizes the weights needed to reproduce the I/O. E.g.","metadata":{"_uuid":"81ba71bfc1b0630cab79c46abdc5e470366551ef","_cell_guid":"adb77d83-8174-4afa-988f-3789346d342b"}},{"cell_type":"code","source":"import numpy as np\nnp.random.seed(0)\n\ndef sigmoid(x): # Returns values that sums to one.\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(sx):\n    # See https://math.stackexchange.com/a/1225116\n    return sx * (1 - sx)\n\n# Cost functions.\ndef cost(predicted, truth):\n    return truth - predicted\n\nxor_input = np.array([[0,0], [0,1], [1,0], [1,1]])\nxor_output = np.array([[0,1,1,0]]).T\n\n# Lets drop the last row of data and use that as unseen test.\nX = xor_input\nY = xor_output\n\n# Define the shape of the weight vector.\nnum_data, input_dim = X.shape\n# Lets set the dimensions for the intermediate layer.\nhidden_dim = 5\n# Initialize weights between the input layers and the hidden layer.\nW1 = np.random.random((input_dim, hidden_dim))\n\n# Define the shape of the output vector. \noutput_dim = len(Y.T)\n# Initialize weights between the hidden layers and the output layer.\nW2 = np.random.random((hidden_dim, output_dim))\n\nnum_epochs = 10000\nlearning_rate = 1.0\n\nfor epoch_n in range(num_epochs):\n    layer0 = X\n    # Forward propagation.\n    \n    # Inside the perceptron, Step 2. \n    layer1 = sigmoid(np.dot(layer0, W1))\n    layer2 = sigmoid(np.dot(layer1, W2))\n\n    # Back propagation (Y -> layer2)\n    \n    # How much did we miss in the predictions?\n    layer2_error = cost(layer2, Y)\n    # In what direction is the target value?\n    # Were we really close? If so, don't change too much.\n    layer2_delta = layer2_error * sigmoid_derivative(layer2)\n\n    \n    # Back propagation (layer2 -> layer1)\n    # How much did each layer1 value contribute to the layer2 error (according to the weights)?\n    layer1_error = np.dot(layer2_delta, W2.T)\n    layer1_delta = layer1_error * sigmoid_derivative(layer1)\n    \n    # update weights\n    W2 +=  learning_rate * np.dot(layer1.T, layer2_delta)\n    W1 +=  learning_rate * np.dot(layer0.T, layer1_delta)","metadata":{"_uuid":"86022c1b3e1d7dfb8268706b23b51ae529b9808d","_cell_guid":"b5b2584f-22fa-4124-9445-44afa2efef2e","execution":{"iopub.status.busy":"2021-10-16T21:49:48.452772Z","iopub.execute_input":"2021-10-16T21:49:48.453123Z","iopub.status.idle":"2021-10-16T21:49:49.036029Z","shell.execute_reply.started":"2021-10-16T21:49:48.453072Z","shell.execute_reply":"2021-10-16T21:49:49.035050Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"# Given all data points, XOR is memorizable\n\nWe see that we've fully trained the network to memorize the outputs for XOR:","metadata":{"_uuid":"e9578076eef581456974ab314e0d40bb9ff47f35","_cell_guid":"70f0fe58-e92d-4f43-be70-80ab45db4548"}},{"cell_type":"code","source":"for x, y in zip(X, Y):\n    layer1_prediction = sigmoid(np.dot(W1.T, x)) # Feed the unseen input into trained W.\n    prediction = layer2_prediction = sigmoid(np.dot(W2.T, layer1_prediction)) # Feed the unseen input into trained W.\n    print(int(prediction > 0.5), y)","metadata":{"_uuid":"6abfa6c57354d72bb107cae5636c623bdccdfbb9","_cell_guid":"84c726bd-a30c-4c36-a741-b4b47ff7b019","execution":{"iopub.status.busy":"2021-10-16T21:49:49.037353Z","iopub.execute_input":"2021-10-16T21:49:49.037639Z","iopub.status.idle":"2021-10-16T21:49:49.047460Z","shell.execute_reply.started":"2021-10-16T21:49:49.037591Z","shell.execute_reply":"2021-10-16T21:49:49.046454Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# What if we drop one data point?\n\nBut if we retrain the parameters (W1 and W2) without one of the data points, i.e.\n\n```\nxor_input = np.array([[0,0], [0,1], [1,0], [1,1]])\nxor_output = np.array([[0,1,1,0]]).T\n\n# Lets drop the last row of data and use that as unseen test.\nX = xor_input[:-1]\nY = xor_output[:-1]\n```\n\nAnd with the rest of the same code, regardless of how I change the hyperparameters, it's un-able to learn the XOR function and reproduce the I/O. ","metadata":{"_uuid":"3685710a7a862efe6c7c92a5653b81b8c2af25b4","_cell_guid":"6e2f0b24-fc48-4195-bf9e-e36f7f307f66"}},{"cell_type":"code","source":"import numpy as np\nnp.random.seed(0)\n\ndef sigmoid(x): # Returns values that sums to one.\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(sx):\n    # See https://math.stackexchange.com/a/1225116\n    return sx * (1 - sx)\n\n# Cost functions.\ndef cost(predicted, truth):\n    return truth - predicted\n\nxor_input = np.array([[0,0], [0,1], [1,0], [1,1]])\nxor_output = np.array([[0,1,1,0]]).T\n\n# Lets drop the last row of data and use that as unseen test.\nX = xor_input[:-1]\nY = xor_output[:-1]\n\n# Define the shape of the weight vector.\nnum_data, input_dim = X.shape\n# Lets set the dimensions for the intermediate layer.\nhidden_dim = 5\n# Initialize weights between the input layers and the hidden layer.\nW1 = np.random.random((input_dim, hidden_dim))\n\n# Define the shape of the output vector. \noutput_dim = len(Y.T)\n# Initialize weights between the hidden layers and the output layer.\nW2 = np.random.random((hidden_dim, output_dim))\n\nnum_epochs = 10000\nlearning_rate = 1.0\n\nfor epoch_n in range(num_epochs):\n    layer0 = X\n    # Forward propagation.\n    \n    # Inside the perceptron, Step 2. \n    layer1 = sigmoid(np.dot(layer0, W1))\n    layer2 = sigmoid(np.dot(layer1, W2))\n\n    # Back propagation (Y -> layer2)\n    \n    # How much did we miss in the predictions?\n    layer2_error = cost(layer2, Y)\n    # In what direction is the target value?\n    # Were we really close? If so, don't change too much.\n    layer2_delta = layer2_error * sigmoid_derivative(layer2)\n\n    \n    # Back propagation (layer2 -> layer1)\n    # How much did each layer1 value contribute to the layer2 error (according to the weights)?\n    layer1_error = np.dot(layer2_delta, W2.T)\n    layer1_delta = layer1_error * sigmoid_derivative(layer1)\n    \n    # update weights\n    W2 +=  learning_rate * np.dot(layer1.T, layer2_delta)\n    W1 +=  learning_rate * np.dot(layer0.T, layer1_delta)","metadata":{"_uuid":"f772a121daf127a2bba72e8651fdff293edc3f90","_cell_guid":"a1efb113-6a96-4645-85e7-164cffa0c88a","execution":{"iopub.status.busy":"2021-10-16T21:49:49.049374Z","iopub.execute_input":"2021-10-16T21:49:49.049906Z","iopub.status.idle":"2021-10-16T21:49:49.583501Z","shell.execute_reply.started":"2021-10-16T21:49:49.049805Z","shell.execute_reply":"2021-10-16T21:49:49.582582Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"for x, y in zip(xor_input, xor_output):\n    layer1_prediction = sigmoid(np.dot(W1.T, x)) # Feed the unseen input into trained W.\n    prediction = layer2_prediction = sigmoid(np.dot(W2.T, layer1_prediction)) # Feed the unseen input into trained W.\n    print(int(prediction > 0.5), y)","metadata":{"_uuid":"8b792538e199dcb9d34eb5513f7b5a8617c9eba8","_cell_guid":"ebc5e351-87cf-427a-ab42-2a36a436bc13","execution":{"iopub.status.busy":"2021-10-16T21:49:49.584974Z","iopub.execute_input":"2021-10-16T21:49:49.585380Z","iopub.status.idle":"2021-10-16T21:49:49.595539Z","shell.execute_reply.started":"2021-10-16T21:49:49.585310Z","shell.execute_reply":"2021-10-16T21:49:49.594723Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"# What if we shuffle the inputs?\n\nEven if we shuffle the in-/outputs, we won't be able to fully trained an XOR.\n\n```\n# Shuffle the order of the inputs\n_temp = list(zip(X, Y))\nrandom.shuffle(_temp)\nxor_input_shuff, xor_output_shuff = map(np.array, zip(*_temp))\n```\n\nI.e.","metadata":{"_uuid":"e29542726d349305426261ba2562aecd00f9328b","_cell_guid":"eb9358bc-0214-40ab-8334-fe0fb20f0927"}},{"cell_type":"code","source":"import numpy as np\nimport random\nnp.random.seed(0)\n\ndef sigmoid(x): # Returns values that sums to one.\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(sx):\n    # See https://math.stackexchange.com/a/1225116\n    return sx * (1 - sx)\n\n# Cost functions.\ndef cost(predicted, truth):\n    return truth - predicted\n\nX = xor_input = np.array([[0,0], [0,1], [1,0], [1,1]])\nY = xor_output = np.array([[0,1,1,0]]).T\n\n# Shuffle the order of the inputs\n_temp = list(zip(X, Y))\nrandom.shuffle(_temp)\nxor_input_shuff, xor_output_shuff = map(np.array, zip(*_temp))\n\n# Lets drop the last row of data and use that as unseen test.\nX = xor_input_shuff[:-1]\nY = xor_output_shuff[:-1]\n\n# Define the shape of the weight vector.\nnum_data, input_dim = X.shape\n# Lets set the dimensions for the intermediate layer.\nhidden_dim = 5\n# Initialize weights between the input layers and the hidden layer.\nW1 = np.random.random((input_dim, hidden_dim))\n\n# Define the shape of the output vector. \noutput_dim = len(Y.T)\n# Initialize weights between the hidden layers and the output layer.\nW2 = np.random.random((hidden_dim, output_dim))\n\nnum_epochs = 10000\nlearning_rate = 1.0\n\nfor epoch_n in range(num_epochs):\n    layer0 = X\n    # Forward propagation.\n    \n    # Inside the perceptron, Step 2. \n    layer1 = sigmoid(np.dot(layer0, W1))\n    layer2 = sigmoid(np.dot(layer1, W2))\n\n    # Back propagation (Y -> layer2)\n    \n    # How much did we miss in the predictions?\n    layer2_error = cost(layer2, Y)\n    # In what direction is the target value?\n    # Were we really close? If so, don't change too much.\n    layer2_delta = layer2_error * sigmoid_derivative(layer2)\n\n    \n    # Back propagation (layer2 -> layer1)\n    # How much did each layer1 value contribute to the layer2 error (according to the weights)?\n    layer1_error = np.dot(layer2_delta, W2.T)\n    layer1_delta = layer1_error * sigmoid_derivative(layer1)\n    \n    # update weights\n    W2 +=  learning_rate * np.dot(layer1.T, layer2_delta)\n    W1 +=  learning_rate * np.dot(layer0.T, layer1_delta)","metadata":{"_uuid":"6f6611cb93ee59375e1b1aacc713af8414c50229","_cell_guid":"51d33aa2-de57-42d2-8be3-e2adb801fded","execution":{"iopub.status.busy":"2021-10-16T21:49:49.596785Z","iopub.execute_input":"2021-10-16T21:49:49.597122Z","iopub.status.idle":"2021-10-16T21:49:50.158464Z","shell.execute_reply.started":"2021-10-16T21:49:49.597068Z","shell.execute_reply":"2021-10-16T21:49:50.157699Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"for x, y in zip(xor_input, xor_output):\n    layer1_prediction = sigmoid(np.dot(W1.T, x)) # Feed the unseen input into trained W.\n    prediction = layer2_prediction = sigmoid(np.dot(W2.T, layer1_prediction)) # Feed the unseen input into trained W.\n    print(x, int(prediction > 0.5), y)","metadata":{"_uuid":"ead38e6c84a4f083e66de66f9d39132c1812794a","_cell_guid":"e045cd5e-c2f0-41b9-81b6-794166d5fbd7","execution":{"iopub.status.busy":"2021-10-16T21:49:50.159763Z","iopub.execute_input":"2021-10-16T21:49:50.160054Z","iopub.status.idle":"2021-10-16T21:49:50.170442Z","shell.execute_reply.started":"2021-10-16T21:49:50.160004Z","shell.execute_reply":"2021-10-16T21:49:50.169605Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"What does it mean by MLP solving XOR?\n====\n\nSo when the literature states that the multi-layered perceptron (Aka the basic deep learning) solves XOR, **Does it mean that** \n\n - it can fully learn and memorize the weights given the fully set of in-/outputs \n - but cannot generalize the XOR problem given that one of data point is missing?\n \n \n BTW, **which is the appropriate literature to cite when it comes to saying deep learning solves XOR?**","metadata":{"_uuid":"ac6ec5192c903f488d57bec4f6923cefe40eabb2","_cell_guid":"d30036e2-95d2-4967-8371-afc8ac627462"}},{"cell_type":"markdown","source":"# After some enlightenment\n\nfrom Yoav's tweet:  https://twitter.com/yoavgo/status/960405025351700480 ","metadata":{"_uuid":"66c02d4bf53fabea5e41ca6d1756aa33200f7c54","_cell_guid":"af842b23-bd6f-4a6b-a14e-81bd5b78e18e"}},{"cell_type":"code","source":"import random\nrandom.seed(0)\n\ndef generate_zero():\n    return random.uniform(0, 49) / 100\n\ndef generate_one():\n    return random.uniform(50, 100) / 100\n\n\ndef generate_xor_XY(num_data_points):\n    Xs, Ys = [], []\n    for _ in range(num_data_points):\n        # xor(0, 0) -> 0 \n        Xs.append([generate_zero(), generate_zero()]); Ys.append([0])\n        # xor(1, 0) -> 1\n        Xs.append([generate_one(), generate_zero()]); Ys.append([1])\n        # xor(0, 1) -> 1\n        Xs.append([generate_zero(), generate_one()]); Ys.append([1])\n        # xor(1, 1) -> 0\n        Xs.append([generate_one(), generate_one()]); Ys.append([0])\n    return Xs, Ys\n","metadata":{"_uuid":"7b6a8c624c48902748a11f327165229d43bd8896","_cell_guid":"25c9b701-d38a-405a-ba93-c4e56e6c63eb","execution":{"iopub.status.busy":"2021-10-16T21:49:50.171877Z","iopub.execute_input":"2021-10-16T21:49:50.172193Z","iopub.status.idle":"2021-10-16T21:49:50.200604Z","shell.execute_reply.started":"2021-10-16T21:49:50.172137Z","shell.execute_reply":"2021-10-16T21:49:50.199553Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"X, Y = generate_xor_XY(100)\nX = np.array(X)\nY = np.array(Y)","metadata":{"_uuid":"fe7420958bdb51618069b1f5121eaf127136cf47","_cell_guid":"5c6d1a5a-0222-4a6b-891d-1733b335c534","execution":{"iopub.status.busy":"2021-10-16T21:49:50.202128Z","iopub.execute_input":"2021-10-16T21:49:50.202499Z","iopub.status.idle":"2021-10-16T21:49:50.214059Z","shell.execute_reply.started":"2021-10-16T21:49:50.202430Z","shell.execute_reply":"2021-10-16T21:49:50.212992Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# First 20 instance of new data.\nfor i, (x, y) in enumerate(zip(X, Y)):\n    if i > 20:\n        break\n    print(x, [int(_x > 0.5) for _x in x],  y)\n    ","metadata":{"_uuid":"babcf447537e830f687e4471aa6118e00f33af87","_cell_guid":"6c749a6a-d537-4a83-9828-7e2cb0dec0a1","execution":{"iopub.status.busy":"2021-10-16T21:49:50.215588Z","iopub.execute_input":"2021-10-16T21:49:50.216000Z","iopub.status.idle":"2021-10-16T21:49:50.242381Z","shell.execute_reply.started":"2021-10-16T21:49:50.215860Z","shell.execute_reply":"2021-10-16T21:49:50.241329Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport random\nnp.random.seed(0)\n\ndef sigmoid(x): # Returns values that sums to one.\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(sx):\n    # See https://math.stackexchange.com/a/1225116\n    return sx * (1 - sx)\n\n# Cost functions.\ndef cost(predicted, truth):\n    return truth - predicted\n\n# Shuffle the order of the inputs\n_temp = list(zip(X, Y))\nrandom.shuffle(_temp)\nxor_input_shuff, xor_output_shuff = map(np.array, zip(*_temp))\n\n# Lets split the data to 90-10. \ntrain_split = int(len(X) / 100 * 90)\nX_train = xor_input_shuff[:train_split]\nY_train = xor_output_shuff[:train_split]\n\nX_test = xor_input_shuff[train_split:]\nY_test = xor_output_shuff[train_split:]\n\n# Define the shape of the weight vector.\nnum_data, input_dim = X_train.shape\n# Lets set the dimensions for the intermediate layer.\nhidden_dim = 5\n# Initialize weights between the input layers and the hidden layer.\nW1 = np.random.random((input_dim, hidden_dim))\n\n# Define the shape of the output vector. \noutput_dim = len(Y_train.T)\n# Initialize weights between the hidden layers and the output layer.\nW2 = np.random.random((hidden_dim, output_dim))\n\nnum_epochs = 2000\nlearning_rate = 0.03\n\nfor epoch_n in range(num_epochs):\n    layer0 = X_train\n    # Forward propagation.\n    \n    # Inside the perceptron, Step 2. \n    layer1 = sigmoid(np.dot(layer0, W1))\n    layer2 = sigmoid(np.dot(layer1, W2))\n\n    # Back propagation (Y -> layer2)\n    \n    # How much did we miss in the predictions?\n    layer2_error = cost(layer2, Y_train)\n    # In what direction is the target value?\n    # Were we really close? If so, don't change too much.\n    layer2_delta = layer2_error * sigmoid_derivative(layer2)\n\n    \n    # Back propagation (layer2 -> layer1)\n    # How much did each layer1 value contribute to the layer2 error (according to the weights)?\n    layer1_error = np.dot(layer2_delta, W2.T)\n    layer1_delta = layer1_error * sigmoid_derivative(layer1)\n    \n    # update weights\n    W2 +=  learning_rate * np.dot(layer1.T, layer2_delta)\n    W1 +=  learning_rate * np.dot(layer0.T, layer1_delta)","metadata":{"_uuid":"41e63743fa11e31dc95c871dcb39b36f5fb592ef","_cell_guid":"03a3135c-e90b-495c-bf0f-ff3dd9c2ba24","execution":{"iopub.status.busy":"2021-10-16T21:49:50.243812Z","iopub.execute_input":"2021-10-16T21:49:50.244320Z","iopub.status.idle":"2021-10-16T21:49:50.650320Z","shell.execute_reply.started":"2021-10-16T21:49:50.244258Z","shell.execute_reply":"2021-10-16T21:49:50.649674Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"accurate = 0\nfor x, y in zip(X_test, Y_test):\n    layer1_prediction = sigmoid(np.dot(W1.T, x)) # Feed the unseen input into trained W.\n    prediction = layer2_prediction = sigmoid(np.dot(W2.T, layer1_prediction)) # Feed the unseen input into trained W.\n    print(x, [int(_x > 0.5) for _x in x], int(prediction > 0.5), y)\n    accurate += int(prediction > 0.5) == y\n    \n#print(X_test, Y_test)\nprint(W1.T)\nprint(W2.T)","metadata":{"_uuid":"c0f5f895f9a036ce8a46c1538fe37978a096e8c2","_cell_guid":"16649a1b-104e-4951-8e69-1e03f365b903","execution":{"iopub.status.busy":"2021-10-16T21:49:50.651519Z","iopub.execute_input":"2021-10-16T21:49:50.652034Z","iopub.status.idle":"2021-10-16T21:49:50.705389Z","shell.execute_reply.started":"2021-10-16T21:49:50.651979Z","shell.execute_reply":"2021-10-16T21:49:50.704405Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"print(accurate/len(X_test))","metadata":{"_uuid":"c7cc0b41abdd9e25e0d81561a5145d3d73bd0607","_cell_guid":"a353da8f-74db-4ff9-8c89-4944ba80f944","execution":{"iopub.status.busy":"2021-10-16T21:49:50.706343Z","iopub.execute_input":"2021-10-16T21:49:50.706554Z","iopub.status.idle":"2021-10-16T21:49:50.711476Z","shell.execute_reply.started":"2021-10-16T21:49:50.706523Z","shell.execute_reply":"2021-10-16T21:49:50.710654Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"e80e86eaec7b5e20c3050a8c54fd239249bfdc44","_cell_guid":"065f0d68-0f89-44cc-ba79-753bc0cb5650","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"cd56ac416a81b0068db6115666a089cc8fc04060","_cell_guid":"89711e2e-28ff-405e-9f64-ee27857d9f7a","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"7ed38482a655cf9a55e4882c7c5b2fa98691fdd9","_cell_guid":"bdfe6117-d2ae-4584-8736-e0ba2a5b6077","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"b9fb434deb70578b3be3dbdeaa03f97539c31698","_cell_guid":"23155d77-06cc-4027-a89c-505578b8bcb8","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"7c0ae28fe7307e7982ccfdb57aed88299d28bbd7","_cell_guid":"4fdce1f2-5b77-4fd5-8c54-94a5211e5925","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"d13908e9f4131ddddd20ccd0c966f5d3bd21eb7d","_cell_guid":"063aacc5-99aa-473e-8c50-294c75c3dc71","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"e1e211cd9afbd5b5318e706ce164afc152978418","_cell_guid":"cfef798e-be84-45a5-b95f-7cb9df7657e7","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"e8efbc54ff1257bb73957a7931ec9c8aa2768833","_cell_guid":"17b20e04-3ce7-4f95-ac2c-623e1dcb3085","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"60a8c0c7478640f1a74ca65663b96b94a039899a","_cell_guid":"fada63bb-f299-4af1-a5cf-06c63bf1cb17","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"ea4f4752feb5bdd9086cab74ae1ebc76a8e15fba","_cell_guid":"024d5a98-dd04-43e8-b006-4858da8753c2","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]}]}